{
 "metadata": {
  "name": "",
  "signature": "sha256:9d5098a937c561c3a7aa7192e0ea7689596f484a5064cd3699c931fd772a6c31"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "\\renewcommand{\\max} [1] {{\\underset{#1}{\\mathrm{max}~}}}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# In-depth supplementary material:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A Support Vector Machine (SVM) is a classifier that attempts to separate classes of data with a plane. The location of the plane in the input space is selected based on a set of training examples, $\\mathbf{x}_i \\in \\Re^\\text{d}$ $(i=1,2,\\dots,N)$, labelled with a binary variable, $\\mathbf{y}_i$. To query the label of a new point, the algorithm simply determines on which side of the plane the point sits, $\\text{sgn}\\big[f(\\mathbf{x})\\big]$ where $f(\\mathbf{x})$ represents the discrimant function associated with the hyperplane\n",
      "\n",
      "\\begin{equation}\n",
      "f(\\mathbf{x})=\\mathbf{w}\\Phi(\\mathbf{x})+b.\n",
      "\\end{equation}\n",
      "\n",
      "Here, $\\mathbf{w}$, $\\mathbf{\\Phi(\\cdot)}$ and $b$ denote the weight vector, the basis function and the bias term, respectively. In two dimensions, this amounts to dividing the input space into two sections with a straight line. In higher dimensions, the division is determined by a plane (or hyperplane in 4 or more dimensions). Typically, the original data is projected into a higher dimensional feature space, $\\Re^{\\text{d}^*}$ where $d^* > d$, using a kernel function $\\Phi(\\mathbf{x})$ to produce more flexible decision boundaries in the lower dimensional input space.\n",
      "\n",
      "The weight vector $\\mathbf{w}$ is optimised using a cost function:\n",
      "\n",
      "\\begin{equation}\n",
      "\\Psi(\\mathbf{w,\\xi})=\\frac{1}{2}\\|\\mathbf{w}\\|^2+C\\sum^{N}_{i=1}\\xi_i\n",
      "\\end{equation}\n",
      "\n",
      "subject to the constraints:\n",
      "\n",
      "\\begin{equation}\n",
      "y_i(\\mathbf{w}\\Phi(\\mathbf{x})+b) \\geqslant 1-\\xi_i\n",
      "\\end{equation}\n",
      "\n",
      "and\n",
      "\\begin{equation}\n",
      "\\xi_i \\geqslant 0 \\; \\; \\; i=1,2,\\dots,N.\n",
      "\\end{equation}\n",
      "\n",
      "Here, $\\xi$ acts as a slack variable which is required when the data is non-separable. $C$ controls the penalty incurred by the model for ignoring misclassified points. \n",
      "\n",
      "Finally, the weights $\\mathbf{w}$ can be found from\n",
      "\n",
      "\\begin{equation}\n",
      "\\mathbf{w} = \\sum^{N}_{i=1}\\alpha_i y_i \\mathbf{x}_i,\n",
      "\\end{equation}\n",
      "\n",
      "where $\\alpha_i$ comes from reformulating the cost function through a Lagrange function and finding the Lagrange multipliers via a dual optimization:\n",
      "\n",
      "\n",
      "\\begin{align}\n",
      "\\max{\\alpha} W(\\alpha) = \\sum^N_{i=1}\\alpha_i - \\frac{1}{2}\\sum^{N}_{i=1}\\alpha_i\\alpha_jy_iy_j\\Phi(\\mathbf{x}_i)\\Phi(\\mathbf{x}_j)\n",
      "\\end{align}\n",
      "\n",
      "subject to the constraint $0 \\leqslant \\alpha_i \\leqslant C$ for all $i=1,2,\\dots,N$. The computational cost of determining the dot product $\\Phi(\\mathbf{x}_i)\\Phi(\\mathbf{x}_j)$ can be large for large $d^*$. Fortunately there exist kernel functions, $k(\\mathbf{x}_i,\\mathbf{x}_j)$, which return the dot product directly without the need to first calculate its factors.\n",
      "\n",
      "The SVM in this paper was trained using five-fold cross-validation. The key parameters, $C$ and $\\gamma$ (a coefficient of the kernel function), were optimised to maximise the model's predictive power on observations that were held-out during training. The final values employed for the results presented were $C = 10$ and $\\gamma = 0.1$.\n",
      "\n",
      "To handle the multi-label nature of the data, a one-versus-one approach was adopted. A separate SVM was trained for each pair of labels resulting in $L(L-1)$ different classifers, where $L$ is the number of labels. During prediction, each classifier was evaluated at the query location and the most frequently returned label was used as the model's output.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " # Simplified Version:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A Support Vector Machine (SVM) is a classifier that attempts to separate classes of data mapped to a space where it can be separated by a hyperplane.\n",
      "\n",
      "The SVM in this paper was trained using five-fold cross-validation. The key parameters, $C$ (which controls the penalty incurred by the model for ignoring misclassified points) and $\\gamma$ (a coefficient of the SVM's kernel function), were optimised to maximise the model's predictive power on observations that were held-out during training. The final values employed for the results presented were $C = 10$ and $\\gamma = 0.1$.\n",
      "\n",
      "To handle the multi-label nature of the data, a one-versus-one approach was adopted. A separate SVM was trained for each pair of labels resulting in $L(L-1)$ different classifers, where $L$ is the number of labels. During prediction, each classifier was evaluated at the query location and the most frequently returned label was used as the model's output.\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}